{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1: Get the Style code, color, quantity, and size from the main sheet into a new sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n",
      "Selected columns have been written to ../../data/main/filtered_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = '../../data/product_data/main_data.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Rename specific columns\n",
    "df = df.rename(columns={\n",
    "    'STYLE NO.': 'Code',\n",
    "    # 'STYLE NO.': 'Handle',\n",
    "    # 'Handle': 'Handle',\n",
    "    'SIZE_US': 'Size',\n",
    "    'COLOR': 'Color',\n",
    "    'QUANTITY': 'Quantity',\n",
    "    'BRAND NAME': 'Brand',\n",
    "    'CATEGORIES': 'Category'\n",
    "})\n",
    "\n",
    "# Define validation criteria for each column\n",
    "\n",
    "\n",
    "def validate_row(row):\n",
    "    # Check for missing or blank values\n",
    "    if pd.isnull(row['Brand']) or pd.isnull(row['Code']) or pd.isnull(row['Color']) or pd.isnull(row['Quantity']) or pd.isnull(row['Size']):\n",
    "        return False\n",
    "    if row['Brand'].strip() == '' or row['Code'].strip() == '' or row['Color'].strip() == '':\n",
    "        return False\n",
    "\n",
    "    # Check for correct data types\n",
    "    if not isinstance(row['Brand'], str) or not isinstance(row['Code'], str) or not isinstance(row['Color'], str):\n",
    "        return False\n",
    "    try:\n",
    "        float(row['Quantity'])\n",
    "        float(row['Size'])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Apply validation to each row\n",
    "valid_rows = df.apply(validate_row, axis=1)\n",
    "\n",
    "# Filter the DataFrame to retain only valid rows\n",
    "filtered_df = df[valid_rows]\n",
    "# # Load the list of handles to discard from another CSV file\n",
    "# handles_file_path = '../../data/product_data/Homecoming.csv'  # Replace with your actual file path\n",
    "# handles_df = pd.read_csv(handles_file_path)\n",
    "# handles_to_discard = handles_df['Handle'].tolist()\n",
    "# filtered_df = filtered_df[~filtered_df['Handle'].isin(handles_to_discard)]\n",
    "\n",
    "# Select the specified columns based on the new names\n",
    "# selected_columns = ['Handle', 'Brand', 'Color', 'Quantity', 'Size', 'Category', 'Code']\n",
    "selected_columns = ['Code', 'Brand', 'Color', 'Quantity', 'Size', 'Category']\n",
    "new_df = filtered_df[selected_columns]\n",
    "\n",
    "# Write the filtered data to a new CSV file\n",
    "# output_file = '../../scripts/catalog/filtered_data.csv'\n",
    "output_file = '../../data/main/filtered_data.csv'\n",
    "new_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print number of unique Codes\n",
    "print(new_df['Code'].nunique())\n",
    "# print(new_df['Code'].nunique())\n",
    "print(f\"Selected columns have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 2: Group this sheet by style code, color, and size and sum of quantity by group to get the total quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Codes in the original DataFrame: 481\n",
      "Number of Codes after aggregation: 481\n",
      "Number of Mapped after aggregation: 481\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(\"../../data/main/filtered_data.csv\")\n",
    "\n",
    "# Ensure Quantity is numeric, coercing errors to NaN\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
    "\n",
    "# Fill NaN values in Quantity with 0 (or handle as needed)\n",
    "df['Quantity'] = df['Quantity'].fillna(0)\n",
    "\n",
    "# First aggregation: Sum quantities based on Code, Color, and Size\n",
    "aggregated_df = df.groupby(['Code', 'Color', 'Size'], as_index=False).agg({\n",
    "    'Brand': 'first',  # Assuming you want to keep the first Brand name found\n",
    "    'Quantity': 'sum',\n",
    "    'Category': 'first',\n",
    "#     'Code': 'first'\n",
    "})\n",
    "\n",
    "# Print the number of unique codes after aggregation\n",
    "print(f\"Number of unique Codes in the original DataFrame: {\n",
    "      df['Code'].nunique()}\")\n",
    "print(f\"Number of Codes after aggregation: {\n",
    "      aggregated_df['Code'].nunique()}\")\n",
    "print(f\"Number of Mapped after aggregation: {\n",
    "      aggregated_df['Code'].nunique()}\")\n",
    "\n",
    "# Write the aggregated data to a new CSV file\n",
    "aggregated_df.to_csv('../../data/main/step_2_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3: Get the prices of available product codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n",
      "Updated file has been saved to ../../data/main/step_3_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "# Replace with your actual file path\n",
    "file1_path = '../../data/product_data/combined_prices.csv'\n",
    "file1 = pd.read_csv(file1_path)\n",
    "\n",
    "# Load the second CSV file\n",
    "# Replace with your actual file path\n",
    "file2_path = '../../data/main/step_2_output.csv'\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Merge the two DataFrames on the 'Code' column to get the 'Price' column from file1\n",
    "# merged_df = pd.merge(file2, file1[['Handle', 'Price']], on='Handle', how='left')\n",
    "merged_df = pd.merge(file2, file1[['Code', 'Price']], on='Code', how='left')\n",
    "\n",
    "# Update the 'Price' column in file2 with the values from the merged DataFrame\n",
    "file2['Price'] = merged_df['Price']\n",
    "\n",
    "# Save the updated file2 DataFrame to a new CSV file\n",
    "# Replace with your desired output file path\n",
    "output_file_path = '../../data/main/step_3_output.csv'\n",
    "\n",
    "print(merged_df['Code'].nunique())\n",
    "file2.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated file has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 4: Combine title, description, keywords, and tags with this new sheet based on product code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n",
      "Merged file has been saved to ../../data/main/step_4_output.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the first CSV file (file_1)\n",
    "# # Replace with the actual file path\n",
    "# file1_path = '../../data/product_data/chatgtp_data_sheet.csv'\n",
    "# file1 = pd.read_csv(file1_path)\n",
    "# # removing copies\n",
    "# file1 = file1.drop_duplicates(subset=['Handle'], keep='first')\n",
    "# # Load the second CSV file (file_2)\n",
    "# # Replace with the actual file path\n",
    "# file2_path = '../../data/main/step_3_output.csv'\n",
    "# file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# # Remove 'Color' column from file_1 if it exists\n",
    "# if 'Color' in file1.columns:\n",
    "#     file1 = file1.drop(columns=['Color'])\n",
    "\n",
    "# # Merge the two DataFrames on the 'Code' column, keeping details from file_1\n",
    "# merged_df = pd.merge(file2, file1, on='Handle', how='inner')\n",
    "\n",
    "# # Define the columns from file_1 to copy to file_2\n",
    "# columns_to_copy = ['Title', 'Long Description', 'Short Description',\n",
    "#                    'Dress Type', 'Occasions', 'Keywords', 'Tags']\n",
    "\n",
    "# # For each column to copy, fill NaN values in merged_df with the corresponding values from file_1\n",
    "# for column in columns_to_copy:\n",
    "#     merged_df[column] = merged_df.groupby('Handle')[column].transform('first')\n",
    "\n",
    "# # Save the merged DataFrame to a new CSV file\n",
    "# # Replace with your desired output file path\n",
    "# output_file_path = '../../data/main/step_4_output.csv'\n",
    "# print(merged_df['Handle'].nunique())\n",
    "# merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# print(f\"Merged file has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 5: Combine image URLs with the product code and add additional URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "Final CSV file with appended URLs has been saved to ../../data/main/step_5_output.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV files\n",
    "# file1_path = '../../data/main/step_4_output.csv'\n",
    "# file2_path = '../../data/product_data/s3_urls.csv'  # Replace with the actual file path\n",
    "\n",
    "# file1 = pd.read_csv(file1_path)\n",
    "# file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# print(file2['Handle'].nunique())\n",
    "\n",
    "# # Merge the two DataFrames on the 'Handle' column\n",
    "# merged_df = pd.merge(file1, file2[['Handle', 'url']], on='Handle', how='left')\n",
    "\n",
    "# # Group URLs by 'Handle' and create a list of URLs for each 'Handle'\n",
    "# url_grouped = file2.groupby('Handle')['url'].apply(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "# # Merge the grouped URLs back to the original DataFrame\n",
    "# final_df = pd.merge(file1, url_grouped, on='Handle', how='left')\n",
    "\n",
    "# # Rename the 'url' column to 'Urls'\n",
    "# final_df = final_df.rename(columns={'url': 'Urls'})\n",
    "\n",
    "# # Discard rows where 'Urls' is NaN (no URL available)\n",
    "# final_df = final_df.dropna(subset=['Urls'])\n",
    "\n",
    "# # Save the final DataFrame to a new CSV file\n",
    "# output_file_path = '../../data/main/step_5_output.csv'\n",
    "# final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# print(f\"Final CSV file with appended URLs has been saved to {output_file_path}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file1_path = '../../data/main/step_3_output.csv'\n",
    "file2_path = '../../data/product_data/shopify_urls.csv'  # Replace with the actual file path\n",
    "\n",
    "file1 = pd.read_csv(file1_path)\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "print(file2['Code'].nunique())\n",
    "\n",
    "# Merge the two DataFrames on the 'Handle' column\n",
    "merged_df = pd.merge(file1, file2[['Code', 'url']], on='Code', how='left')\n",
    "\n",
    "# Group URLs by 'Handle' and create a list of URLs for each 'Handle'\n",
    "url_grouped = file2.groupby('Code')['url'].apply(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "# Merge the grouped URLs back to the original DataFrame\n",
    "final_df = pd.merge(file1, url_grouped, on='Code', how='left')\n",
    "\n",
    "# Rename the 'url' column to 'Urls'\n",
    "final_df = final_df.rename(columns={'url': 'Urls'})\n",
    "\n",
    "# Discard rows where 'Urls' is NaN (no URL available)\n",
    "final_df = final_df.dropna(subset=['Urls'])\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "output_file_path = '../../data/main/step_5_output.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Final CSV file with appended URLs has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 6: Creating shopify csv file to upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n",
      "Shopify CSV file has been saved to ../../data/shopify_sheets/main_shopify.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "# Replace with the actual file path\n",
    "file1_path = '../../data/main/step_5_output.csv'\n",
    "\n",
    "\n",
    "file1 = pd.read_csv(file1_path)\n",
    "\n",
    "\n",
    "# Create Shopify CSV structure\n",
    "\n",
    "\n",
    "shopify_columns = ['Handle', 'Title', 'Body (HTML)', 'Vendor', 'Type', 'Tags', 'Published', 'Option1 Name', 'Option1 Value', 'Option2 Name', 'Option2 Value', 'Variant SKU', 'Variant Grams', 'Variant Inventory Tracker', 'Variant Inventory Qty', 'Variant Inventory Policy', 'Variant Fulfillment Service', 'Variant Price', 'Variant Compare at Price', 'Variant Requires Shipping', 'Variant Taxable', 'Variant Barcode', 'Image Src', 'Image Position',\n",
    "\n",
    "\n",
    "\n",
    "                   'Image Alt Text'\n",
    "\n",
    "\n",
    "\n",
    "                   ]\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "\n",
    "\n",
    "shopify_df = pd.DataFrame(columns=shopify_columns)\n",
    "all_entries = []\n",
    "\n",
    "current_code = None\n",
    "\n",
    "other_images = []\n",
    "\n",
    "\n",
    "# def check_sleeve_preference(combined_list):\n",
    "#     # Define sleeve-related terms\n",
    "#     sleeve_terms = [\"Sleeveless\", \"Short Sleeve\",\n",
    "#                     \"Long Sleeve\", \"Spaghetti Strap\"]\n",
    "\n",
    "#     # Check for the presence of sleeve terms\n",
    "#     found_sleeves = set()\n",
    "#     for term in sleeve_terms:\n",
    "#         # Check each column in the row for the term\n",
    "#         if any(re.search(rf'\\b{term}\\b', col, re.IGNORECASE) for col in combined_list):\n",
    "#             found_sleeves.add(term)\n",
    "\n",
    "#     # Determine the preferred sleeve term\n",
    "#     if \"Spaghetti Strap\" in found_sleeves:\n",
    "#         return \"Spaghetti Strap\"\n",
    "#     elif \"Short Sleeve\" in found_sleeves:\n",
    "#         return \"Short Sleeve\"\n",
    "#     elif found_sleeves:\n",
    "#         # Return any other found term if Spaghetti Strap or Short Sleeve is not found\n",
    "#         return list(found_sleeves)[0]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# Iterate over rows using iterrows()\n",
    "for index, row in file1.iterrows():\n",
    "\n",
    "    # Check if we have encountered a new code\n",
    "\n",
    "    if current_code is not None and current_code != row['Code']:\n",
    "\n",
    "        # Append additional images for the previous code\n",
    "\n",
    "        for idx, url in enumerate(other_images):\n",
    "\n",
    "            image_entry = {'Handle': current_code.lower(),\n",
    "                           'Image Src': url,\n",
    "                           'Image Position': idx + 2, }\n",
    "\n",
    "            all_entries.append(pd.DataFrame([image_entry]))\n",
    "\n",
    "        # Reset the other_images list for the new code\n",
    "\n",
    "        other_images = []\n",
    "\n",
    "    # Set the current code\n",
    "\n",
    "    current_code = row['Code']\n",
    "    # current_code = row['Handle']\n",
    "    category = row['Category'] if pd.notna(row['Category']) else \"Uncategorized\"\n",
    "    category = \"Wedding\" if \"AA\" in row['Code'] else category\n",
    "    # Generate handle from product name\n",
    "\n",
    "    keywords = [str(row['Color']), str(row['Color']) + ' '+ str(category)]\n",
    "    meta_features = \", \".join(keywords)\n",
    "\n",
    "    # description = str(row['Long Description']) if pd.notna(\n",
    "    #     row['Long Description']) else ''\n",
    "\n",
    "    body_html = f'<p>This is a {category} dress by {row['Brand']}.</p>\\n<ul>'\n",
    "\n",
    "    # for i in range(0, len(keywords)):\n",
    "\n",
    "    #     body_html += f'\\n<li>{keywords[i]}</li>'\n",
    "\n",
    "    # body_html += '\\n</ul>'\n",
    "\n",
    "    if isinstance(row['Urls'], str):\n",
    "\n",
    "        all_images = row['Urls'].split(\",\")\n",
    "\n",
    "        first_image, *other_images = all_images\n",
    "\n",
    "    # else:\n",
    "\n",
    "    #     continue\n",
    "\n",
    "    title = row['Brand'] + \" \" + category +\" \" + row['Code'] if category != 'Uncategorized' else row['Brand'] + \" \" + row['Code']\n",
    "    title = title.replace('\"', \"\")\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "\n",
    "    # temp_extract = [row['Occasions'], row['Keywords'],  row['Tags']]\n",
    "\n",
    "    # meta_sleeve_length_type = check_sleeve_preference(temp_extract)\n",
    "\n",
    "    # Create a new row for the main product entry\n",
    "\n",
    "    product_entry = {\n",
    "\n",
    "\n",
    "\n",
    "        'Handle': row['Code'].lower(),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        'Title': title,\n",
    "\n",
    "\n",
    "        'Body (HTML)': body_html,\n",
    "\n",
    "\n",
    "\n",
    "        'Vendor': row['Brand'],\n",
    "\n",
    "\n",
    "\n",
    "        'Type': category,  # You can customize this\n",
    "\n",
    "\n",
    "\n",
    "        'Tags': \",\".join(keywords),\n",
    "\n",
    "\n",
    "\n",
    "        'Published': 'TRUE',\n",
    "\n",
    "\n",
    "\n",
    "        \"Product Category\": \"Apparel & Accessories > Clothing > Dresses\",\n",
    "\n",
    "\n",
    "\n",
    "        'Option1 Name': 'Color',\n",
    "\n",
    "\n",
    "\n",
    "        'Option1 Value': row['Color'],\n",
    "\n",
    "\n",
    "\n",
    "        'Option2 Name': 'Size',\n",
    "\n",
    "\n",
    "\n",
    "        'Option2 Value': row['Size'],\n",
    "\n",
    "\n",
    "\n",
    "        'Variant SKU': f\"{row['Code']}_{row['Color'].upper()}_{row['Size']}\",\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Grams': '',  # Add weight if available\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Inventory Tracker': 'shopify',\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Inventory Qty': row['Quantity'],\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Inventory Policy': 'deny',\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Fulfillment Service': 'manual',\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Price': row['Price'],\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Compare at Price': '',\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Requires Shipping': 'TRUE',\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Taxable': 'TRUE',\n",
    "\n",
    "\n",
    "\n",
    "        'Variant Barcode': '',\n",
    "\n",
    "\n",
    "\n",
    "        'Image Src': first_image,  # First image URL\n",
    "\n",
    "\n",
    "\n",
    "        'Image Position': 1,\n",
    "\n",
    "\n",
    "      'Image Alt Text': row['Brand'] + \" \" + row['Code'].lower() + \" \" + row['Color'],\n",
    "        'metafield.custom.clothing_features': meta_features,\n",
    "        'metafield.custom.dress_occasion': re.sub(r'\\s+', ' ', category),\n",
    "        'metafield.custom.dress_style': row['Code'].upper(),\n",
    "        'metafield.custom.skirt_dress_length_type': category,\n",
    "        # 'metafield.custom.sleeve_length_type':\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    # Add the main product entry to the list\n",
    "\n",
    "    all_entries.append(pd.DataFrame([product_entry]))\n",
    "\n",
    "\n",
    "# Append additional images for the last code\n",
    "\n",
    "\n",
    "if other_images:\n",
    "\n",
    "    for idx, url in enumerate(other_images):\n",
    "\n",
    "        image_entry = {\n",
    "\n",
    "\n",
    "\n",
    "            'Handle': current_code.lower(),\n",
    "\n",
    "\n",
    "\n",
    "            'Image Src': url,\n",
    "\n",
    "\n",
    "\n",
    "            'Image Position': idx + 2,  # Start from position 2\n",
    "\n",
    "\n",
    "\n",
    "        }\n",
    "\n",
    "        all_entries.append(pd.DataFrame([image_entry]))\n",
    "\n",
    "\n",
    "# Concatenate all entries into a single DataFrame\n",
    "\n",
    "\n",
    "shopify_df = pd.concat(all_entries, ignore_index=True)\n",
    "\n",
    "\n",
    "print(shopify_df['Handle'].nunique())\n",
    "output_file_path = '../../data/shopify_sheets/main_shopify.csv'\n",
    "shopify_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Shopify CSV file has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some verificatiosn for understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/sheets/step_5_output.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m file1_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/sheets/step_5_output.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m file2_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/s3_url_images.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the actual file path\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m file1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile1_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m file2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file2_path)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Identify common codes between file1 and file2\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/sheets/step_5_output.csv'"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV files\n",
    "# # Replace with the actual file path\n",
    "# file1_path = '../../data/sheets/step_5_output.csv'\n",
    "# file2_path = '../../data/s3_url_images.csv'  # Replace with the actual file path\n",
    "\n",
    "# file1 = pd.read_csv(file1_path)\n",
    "# file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# # Identify common codes between file1 and file2\n",
    "# common_codes = set(file1['Code']).intersection(set(file2['Product Code']))\n",
    "# print(len(common_codes))\n",
    "# #\n",
    "# # Filter file1 to include only rows with common codes\n",
    "# filtered_file1 = file1[file1['Code'].isin(common_codes)]\n",
    "\n",
    "# # Group by 'Code' and count unique combinations of 'Size' and 'Color'\n",
    "# variant_counts = filtered_file1.groupby('Code').apply(lambda x: x[[\n",
    "#     'Size', 'Color']].drop_duplicates().shape[0], include_groups=False).reset_index(name='Variant Count')\n",
    "\n",
    "# # Print the variant counts\n",
    "# # print(variant_counts)\n",
    "\n",
    "# # Calculate the sum of the variant counts\n",
    "# total_variants = variant_counts['Variant Count'].sum()\n",
    "\n",
    "# # Print the total number of variants\n",
    "# print(f\"Total number of variants: {total_variants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common codes: 338\n",
      "Unique codes in file1: 2\n",
      "{'e2253', 'f570'}\n",
      "Unique codes in file2: 0\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv('../../data/shopify_sheets/main_shopify.csv')\n",
    "df2 = pd.read_csv('../../data/shopify_sheets/products_export.csv')\n",
    "\n",
    "# Convert the 'Code' column to sets for comparison\n",
    "set1 = set(df1['Handle'])\n",
    "set2 = set(df2['Handle'])\n",
    "\n",
    "# Find common and uncommon codes\n",
    "common_codes = set1.intersection(set2)\n",
    "unique_in_file1 = set1.difference(set2)\n",
    "unique_in_file2 = set2.difference(set1)\n",
    "\n",
    "# Print results\n",
    "print(f\"Common codes: {len(common_codes)}\")\n",
    "# print(common_codes)\n",
    "\n",
    "print(f\"Unique codes in file1: {len(unique_in_file1)}\")\n",
    "print(unique_in_file1)\n",
    "\n",
    "print(f\"Unique codes in file2: {len(unique_in_file2)}\")\n",
    "print(unique_in_file2)\n",
    "\n",
    "# Optional: Create DataFrames for common and unique codes and save to new CSV files\n",
    "pd.DataFrame(common_codes, columns=['Code']).to_csv(\n",
    "    'common_codes.csv', index=False)\n",
    "pd.DataFrame(unique_in_file1, columns=['Code']).to_csv(\n",
    "    'unique_in_file1.csv', index=False)\n",
    "pd.DataFrame(unique_in_file2, columns=['Code']).to_csv(\n",
    "    'unique_in_file2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the list of handles to be discarded\n",
    "handles_to_discard = ['handle1', 'handle2', 'handle3']  # Replace with actual handle values you want to discard\n",
    "\n",
    "# Load the CSV file\n",
    "input_file_path = 'path/to/your/input.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Filter the DataFrame to keep only rows where 'Handle' is not in the list of handles to discard\n",
    "filtered_df = df[~df['Handle'].isin(handles_to_discard)]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "output_file_path = 'path/to/your/output.csv'\n",
    "filtered_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered file has been saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "load-data-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
