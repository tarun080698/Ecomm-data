{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1: Get the Style code, color, quantity, and size from the main sheet into a new sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns have been written to ../../data/sheets/step_1_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "input_file = '../../data/sheets/main_sheet.csv'  # Replace with your input file path\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Rename specific columns\n",
    "df = df.rename(columns={\n",
    "    'STYLE NO.': 'Code',\n",
    "    'SIZE_US': 'Size',\n",
    "    'COLOR': 'Color',\n",
    "    'QUANTITY': 'Quantity',\n",
    "    'BRAND NAME': 'Brand'\n",
    "})\n",
    "\n",
    "# Select the specified columns based on the new names\n",
    "selected_columns = ['Brand', 'Code', 'Color', 'Quantity', 'Size']\n",
    "new_df = df[selected_columns]\n",
    "\n",
    "# Save the selected columns to a new CSV file\n",
    "output_file = '../../data/sheets/step_1_output.csv'  # Replace with your desired output file path\n",
    "new_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Selected columns have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 2: Group this sheet by style code, color, and size and sum of quantity by group to get the total quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(\"../../data/sheets/step_1_output.csv\")\n",
    "\n",
    "# Ensure QUANTITY is numeric, coercing errors to NaN\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
    "\n",
    "# Fill NaN values in QUANTITY with 0 (or handle as needed)\n",
    "df['Quantity'] = df['Quantity'].fillna(0)\n",
    "\n",
    "# First aggregation: Sum quantities based on STYLE NO. and COLOR\n",
    "aggregated_df = df.groupby(['Code', 'Color', 'Size'], as_index=False).agg({\n",
    "    'Brand': 'first',\n",
    "    'Quantity': 'sum'\n",
    "})\n",
    "\n",
    "# Second aggregation: Sum quantities based on STYLE NO. to get total quantity for each style\n",
    "style_quantity_df = aggregated_df.groupby('Code', as_index=False).agg({\n",
    "    'Quantity': 'sum'\n",
    "})\n",
    "\n",
    "# print(style_quantity_df['STYLE NO.'].unique())\n",
    "# print(style_quantity_df['STYLE NO.'].nunique())\n",
    "\n",
    "aggregated_df.to_csv('../../data/sheets/step_2_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3: Get the prices of available product codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file has been saved to ../../data/sheets/step_3_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "file1_path = '../../data/transformed_products.csv'  # Replace with your actual file path\n",
    "file1 = pd.read_csv(file1_path)\n",
    "\n",
    "# Load the second CSV file\n",
    "file2_path = '../../data/sheets/step_2_output.csv'  # Replace with your actual file path\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Merge the two DataFrames on the 'Code' column to get the 'Price' column from file1\n",
    "merged_df = pd.merge(file2, file1[['Code', 'Price']], on='Code', how='left')\n",
    "\n",
    "# Update the 'Price' column in file2 with the values from the merged DataFrame\n",
    "file2['Price'] = merged_df['Price']\n",
    "\n",
    "# Save the updated file2 DataFrame to a new CSV file\n",
    "output_file_path = '../../data/sheets/step_3_output.csv'  # Replace with your desired output file path\n",
    "file2.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated file has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 4: Combine title, description, keywords, and tags with this new sheet based on product code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file has been saved to ../../data/sheets/step_4_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file (file_1)\n",
    "file1_path = '../../data/sheets/product_desc.csv'  # Replace with the actual file path\n",
    "file1 = pd.read_csv(file1_path)\n",
    "# removing copies\n",
    "file1 = file1.drop_duplicates(subset=['Code'], keep='first')\n",
    "# Load the second CSV file (file_2)\n",
    "file2_path = '../../data/sheets/step_3_output.csv'  # Replace with the actual file path\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Remove 'Color' column from file_1 if it exists\n",
    "if 'Color' in file1.columns:\n",
    "    file1 = file1.drop(columns=['Color'])\n",
    "\n",
    "# Merge the two DataFrames on the 'Code' column, keeping details from file_1\n",
    "merged_df = pd.merge(file2, file1, on='Code', how='inner')\n",
    "\n",
    "# Define the columns from file_1 to copy to file_2\n",
    "columns_to_copy = ['Long Description', 'Short Description', 'Keywords', 'Tags']\n",
    "\n",
    "# For each column to copy, fill NaN values in merged_df with the corresponding values from file_1\n",
    "for column in columns_to_copy:\n",
    "    merged_df[column] = merged_df.groupby('Code')[column].transform('first')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "output_file_path = '../../data/sheets/step_4_output.csv'  # Replace with your desired output file path\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged file has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 5: Combine image URLs with the product code and add additional URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV file with appended URLs has been saved to ../../data/sheets/step_5_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file1_path = '../../data/sheets/step_4_output.csv'  # Replace with the actual file path\n",
    "file2_path = '../../data/s3_url_images.csv'  # Replace with the actual file path\n",
    "\n",
    "file1 = pd.read_csv(file1_path)\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Rename columns in file2 to match the columns in file1 for merging\n",
    "file2 = file2.rename(columns={'Product Code': 'Code'})\n",
    "\n",
    "# Merge the two DataFrames on the 'Code' column\n",
    "merged_df = pd.merge(file1, file2[['Code', 'url']], on='Code', how='left')\n",
    "\n",
    "# Group URLs by 'Code' and create a list of URLs for each 'Code'\n",
    "url_grouped = file2.groupby('Code')['url'].apply(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "# Merge the grouped URLs back to the original DataFrame\n",
    "final_df = pd.merge(file1, url_grouped, on='Code', how='left')\n",
    "final_df = final_df.rename(columns={'url': 'Urls'})\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "output_file_path = '../../data/sheets/step_5_output.csv'  # Replace with your desired output file path\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Final CSV file with appended URLs has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 6: Creating shopify csv file to upload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "['aa219' 'aa221' 'aa222' 'aa226' 'aa229' 'aa233' 'aa9254' 'aa9256'\n",
      " 'aa9257' 'aa9270' 'aa9298' 'aa9301' 'aa9304' 'aa9306' 'aa9308' 'aa9309'\n",
      " 'aa9310' 'aa9312' 'aa9313' 'aa9314' 'aa9315' 'aa9316' 'aa9317' 'aa9318'\n",
      " 'aa9319' 'aa9320' 'aa9321' 'aa9322' 'aa9326' 'aa9327' 'aa9329' 'aa9330'\n",
      " 'aa9331' 'aa9332' 'aa9335' 'aa9336' 'e1342' 'e1344' 'e1347' 'e1348'\n",
      " 'e1354' 'e1356' 'e1378' 'e1379' 'e1381' 'e1396' 'e1420' 'e1421' 'e1422'\n",
      " 'e1424' 'e1428' 'e1438' 'e1448' 'e1452' 'e1463' 'e1472' 'e1502' 'e1508'\n",
      " 'e1510' 'e1520' 'e1531' 'e1544' 'e1546' 'e1573' 'e1579' 'e1581' 'e1582'\n",
      " 'e1637' 'e1657' 'e1663' 'e1694' 'e1696' 'e1715' 'e1729' 'e1730' 'e1767'\n",
      " 'e1774' 'e1778' 'e1796' 'e1799' 'e1822' 'e1824' 'e1827' 'e1833' 'e1838'\n",
      " 'e1844' 'e1852' 'e1868' 'e1882' 'e1890' 'e1907' 'e1923' 'e1931' 'e1937'\n",
      " 'e1940' 'e1965' 'e1967' 'e1974' 'e1978' 'e1979' 'e1981' 'e1982' 'e1983'\n",
      " 'e1986' 'e1988' 'e2016' 'e2019' 'e2020' 'e2021' 'e2023' 'e2025' 'e2027'\n",
      " 'e2028' 'e2030' 'e2031' 'e2032' 'e2034' 'e2035' 'e2036' 'e2039' 'e2040'\n",
      " 'e2041' 'e2042' 'e2043' 'e2049' 'e2050' 'e2053' 'e2059' 'e2060' 'e2061'\n",
      " 'e2067' 'e2068' 'e2069' 'e2070' 'e2072' 'e2074' 'e2075' 'e2076' 'e2080'\n",
      " 'e2081' 'e2084' 'e2085' 'e2086' 'e2087' 'e2088' 'e2096' 'e2097' 'e2100'\n",
      " 'e2101' 'e2102' 'e2104' 'e2107' 'e2109' 'e2110' 'e2114' 'e2118' 'e2121'\n",
      " 'e2125' 'e2136' 'e2144' 'e2150' 'e2152' 'e2158' 'e2160' 'e2161' 'e2162'\n",
      " 'e2165' 'e2171' 'e2182' 'e2204' 'e2210' 'e2213' 'e2227' 'e2229' 'e2231'\n",
      " 'e2233' 'e2241' 'e2243' 'e2244' 'e2245' 'e2251' 'e2252' 'e2253' 'e2254'\n",
      " 'e2255' 'e2256' 'e2257' 'e2258' 'e2259' 'e2262' 'e2267' 'e2269' 'e2275'\n",
      " 'e2279' 'e2280' 'e2281' 'e2282' 'e2287' 'e2289' 'e2291' 'e2292' 'e2296'\n",
      " 'e2317' 'e2318' 'e2320' 'e2322' 'e2324' 'e2326' 'e2327' 'e2328' 'e2329'\n",
      " 'e2331' 'e2333' 'e2334' 'e2335' 'e2336' 'e2340' 'e2344' 'e2346' 'e2356'\n",
      " 'e2360' 'e2366' 'e2385' 'e2388' 'e2389' 'e2395' 'e2403' 'e2405']\n",
      "Shopify CSV file has been saved to ../../data/sheets/shopify_output_demo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file1_path = '../../data/sheets/step_5_output.csv'  # Replace with the actual file path\n",
    "file1 = pd.read_csv(file1_path)\n",
    "\n",
    "# Create Shopify CSV structure\n",
    "shopify_columns = [\n",
    "    'Handle', 'Title', 'Body (HTML)', 'Vendor', 'Type', 'Tags',\n",
    "    'Published', 'Option1 Name', 'Option1 Value', 'Option2 Name', 'Option2 Value', 'Variant SKU',\n",
    "    'Variant Grams', 'Variant Inventory Tracker', 'Variant Inventory Qty',\n",
    "    'Variant Inventory Policy', 'Variant Fulfillment Service', 'Variant Price',\n",
    "    'Variant Compare at Price', 'Variant Requires Shipping',\n",
    "    'Variant Taxable', 'Variant Barcode', 'Image Src', 'Image Position',\n",
    "    'Image Alt Text'\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "shopify_df = pd.DataFrame(columns=shopify_columns)\n",
    "\n",
    "# Populate Shopify DataFrame with data from file1\n",
    "all_entries = []\n",
    "\n",
    "# Initialize variables to track the current code and its associated images\n",
    "current_code = None\n",
    "other_images = []\n",
    "\n",
    "# Iterate over rows using iterrows()\n",
    "for index, row in file1.iterrows():\n",
    "    # Check if we have encountered a new code\n",
    "    if current_code is not None and current_code != row['Code']:\n",
    "        # Append additional images for the previous code\n",
    "        for idx, url in enumerate(other_images):\n",
    "            image_entry = {\n",
    "                'Handle': current_code.lower(),\n",
    "                'Image Src': url,\n",
    "                'Image Position': idx + 2,  # Start from position 2\n",
    "            }\n",
    "            all_entries.append(pd.DataFrame([image_entry]))\n",
    "\n",
    "        # Reset the other_images list for the new code\n",
    "        other_images = []\n",
    "\n",
    "    # Set the current code\n",
    "    current_code = row['Code']\n",
    "\n",
    "    # Generate handle from product name\n",
    "    keywords = str(row['Keywords']) if pd.notna(row['Keywords']) else ''\n",
    "    if keywords != '':\n",
    "        keywords = keywords.split(\", \")\n",
    "    description = str(row['Long Description']) if pd.notna(row['Long Description']) else ''\n",
    "\n",
    "    body_html = f'<p>{description}</p>\\n<ul>'\n",
    "    for i in range(1, len(keywords) - 1):\n",
    "        body_html += f'\\n<li>{keywords[i]}</li>'\n",
    "    body_html += '\\n</ul>'\n",
    "    product_type = \"Dress\"\n",
    "\n",
    "    if isinstance(row['Urls'], str):\n",
    "        all_images = row['Urls'].split(\",\")\n",
    "        first_image, *other_images = all_images\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Create a new row for the main product entry\n",
    "    product_entry = {\n",
    "        'Handle': row['Code'].lower(),\n",
    "        'Title': row['Brand'] + \" \" + row['Code'],\n",
    "        'Body (HTML)': body_html,\n",
    "        'Vendor': row['Brand'],\n",
    "        'Type': product_type,  # You can customize this\n",
    "        'Tags': row['Tags'],\n",
    "        'Published': 'TRUE',\n",
    "        \"Product Category\": \"Apparel & Accessories > Clothing > Dresses\",\n",
    "        'Option1 Name': 'Size',\n",
    "        'Option1 Value': row['Size'],\n",
    "        'Option2 Name': 'Color',\n",
    "        'Option2 Value': row['Color'],\n",
    "        'Variant SKU': f\"{row['Code'].lower()}_{row['Color'].lower()}_{row['Size']}\",\n",
    "        'Variant Grams': '',  # Add weight if available\n",
    "        'Variant Inventory Tracker': 'shopify',\n",
    "        'Variant Inventory Qty': row['Quantity'],\n",
    "        'Variant Inventory Policy': 'deny',\n",
    "        'Variant Fulfillment Service': 'manual',\n",
    "        'Variant Price': row['Price'],\n",
    "        'Variant Compare at Price': '',\n",
    "        'Variant Requires Shipping': 'TRUE',\n",
    "        'Variant Taxable': 'TRUE',\n",
    "        'Variant Barcode': '',\n",
    "        'Image Src': first_image,  # First image URL\n",
    "        'Image Position': 1,\n",
    "        'Image Alt Text': row['Brand'] + \" \" + row['Code'].lower() + \" \" + row['Filename']\n",
    "    }\n",
    "\n",
    "    # Add the main product entry to the list\n",
    "    all_entries.append(pd.DataFrame([product_entry]))\n",
    "\n",
    "# Append additional images for the last code\n",
    "if other_images:\n",
    "    for idx, url in enumerate(other_images):\n",
    "        image_entry = {\n",
    "            'Handle': current_code.lower(),\n",
    "            'Image Src': url,\n",
    "            'Image Position': idx + 2,  # Start from position 2\n",
    "        }\n",
    "        all_entries.append(pd.DataFrame([image_entry]))\n",
    "\n",
    "# Concatenate all entries into a single DataFrame\n",
    "shopify_df = pd.concat(all_entries, ignore_index=True)\n",
    "\n",
    "print(shopify_df['Handle'].nunique())\n",
    "print(shopify_df['Handle'].unique())\n",
    "\n",
    "# Save the Shopify DataFrame to a new CSV file\n",
    "output_file_path = '../../data/sheets/shopify_output_demo.csv'  # Replace with your desired output file path\n",
    "shopify_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Shopify CSV file has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some verificatiosn for understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique codes in file1: 228\n",
      "Unique product codes in file2: 299\n",
      "Common codes: 0\n",
      "Common codes list: set()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Size', 'Color'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommon codes list: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_codes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Group by 'Code' and count unique combinations of 'Size' and 'Color'\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m variant_counts \u001b[38;5;241m=\u001b[39m \u001b[43mfile1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHandle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mColor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariant Count\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Print the variant counts\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(variant_counts)\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1824\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1826\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series)\n\u001b[0;32m   1827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1829\u001b[0m         ):\n\u001b[0;32m   1830\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1831\u001b[0m                 message\u001b[38;5;241m=\u001b[39m_apply_groupings_depr\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1832\u001b[0m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1835\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1836\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:919\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    918\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 919\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    921\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[122], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommon codes list: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_codes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Group by 'Code' and count unique combinations of 'Size' and 'Color'\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m variant_counts \u001b[38;5;241m=\u001b[39m file1\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHandle\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mColor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariant Count\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Print the variant counts\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(variant_counts)\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tarun\\miniconda3\\envs\\load-data-test\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Size', 'Color'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file1_path = '../../data/sheets/shopify_output_demo.csv'  # Replace with the actual file path\n",
    "file2_path = '../../data/s3_url_images.csv'  # Replace with the actual file path\n",
    "file1 = pd.read_csv(file1_path)\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Get unique values from the 'Code' and 'Product Code' columns\n",
    "codes_file1 = set(file1['Handle'].unique())\n",
    "codes_file2 = set(file2['Product Code'].unique())\n",
    "\n",
    "# Find the intersection of the two sets\n",
    "common_codes = codes_file1.intersection(codes_file2)\n",
    "\n",
    "# Print the number of unique values and the common values\n",
    "print(f\"Unique codes in file1: {len(codes_file1)}\")\n",
    "print(f\"Unique product codes in file2: {len(codes_file2)}\")\n",
    "print(f\"Common codes: {len(common_codes)}\")\n",
    "print(f\"Common codes list: {common_codes}\")\n",
    "\n",
    "\n",
    "# Group by 'Code' and count unique combinations of 'Size' and 'Color'\n",
    "variant_counts = file1.groupby('Handle').apply(lambda x: x[['Size', 'Color']].drop_duplicates().shape[0]).reset_index(name='Variant Count')\n",
    "\n",
    "# Print the variant counts\n",
    "print(variant_counts)\n",
    "\n",
    "\n",
    "# Calculate the sum of the variant counts\n",
    "total_variants = variant_counts['Variant Count'].sum()\n",
    "\n",
    "# Print the total number of variants\n",
    "print(f\"Total number of variants: {total_variants}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of variants: 1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarun\\AppData\\Local\\Temp\\ipykernel_17224\\2822783968.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  variant_counts = filtered_file1.groupby('Code').apply(lambda x: x[['Size', 'Color']].drop_duplicates().shape[0]).reset_index(name='Variant Count')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file1_path = '../../data/sheets/step_5_output.csv'  # Replace with the actual file path\n",
    "file2_path = '../../data/s3_url_images.csv'  # Replace with the actual file path\n",
    "\n",
    "file1 = pd.read_csv(file1_path)\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Identify common codes between file1 and file2\n",
    "common_codes = set(file1['Code']).intersection(set(file2['Product Code']))\n",
    "\n",
    "# Filter file1 to include only rows with common codes\n",
    "filtered_file1 = file1[file1['Code'].isin(common_codes)]\n",
    "\n",
    "# Group by 'Code' and count unique combinations of 'Size' and 'Color'\n",
    "variant_counts = filtered_file1.groupby('Code').apply(lambda x: x[['Size', 'Color']].drop_duplicates().shape[0]).reset_index(name='Variant Count')\n",
    "\n",
    "# Print the variant counts\n",
    "# print(variant_counts)\n",
    "\n",
    "# Calculate the sum of the variant counts\n",
    "total_variants = variant_counts['Variant Count'].sum()\n",
    "\n",
    "# Print the total number of variants\n",
    "print(f\"Total number of variants: {total_variants}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "load-data-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
