{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DESCRIPTION GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your secret using os.getenv\n",
    "OPEN_AI_API = os.getenv(\"OPEN_AI_API\")\n",
    "print(OPEN_AI_API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 24751 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 24740 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Improved descriptions saved. ../../data/product_data/product_data.csv\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "api_key = OPEN_AI_API\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def improve_description(description):\n",
    "    try:\n",
    "        # Call OpenAI API to improve the description\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a product description assistant that writes engaging and high-quality descriptions for fashion items. \"\n",
    "                        \"Each description should be 4-6 sentences, highlighting the style, material, unique features, and ideal occasions \"\n",
    "                        \"for wearing the item. Maintain proper grammar and punctuation. Please do not use similar sentence structure. \"\n",
    "                        \"Try to use different action words for each output and make it more creative.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Please write a detailed and engaging description for the following dress:\\n\\n{description}\"\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        # Extract the improved description from the API response\n",
    "        improved_text = response.choices[0].message.content\n",
    "        return improved_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return description  # Return the original description if there's an error\n",
    "\n",
    "# Load descriptions from CSV\n",
    "input_file = \"../../data/product_data/product_data.csv\"\n",
    "df = pd.read_csv(input_file, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Create a new column for improved descriptions\n",
    "df['ai_description'] = \"\"\n",
    "\n",
    "# Process descriptions, with a counter to limit processing to 10 descriptions\n",
    "# processed_count = 0\n",
    "for index, row in df.iterrows():\n",
    "    description = row['Description']\n",
    "\n",
    "    if pd.notna(description) and description.strip():\n",
    "        improved_desc = improve_description(description)\n",
    "        df.at[index, 'ai_description'] = improved_desc\n",
    "        # processed_count += 1\n",
    "\n",
    "        # if processed_count >= 10:\n",
    "        #     break  # Stop after processing 10 descriptions\n",
    "\n",
    "        time.sleep(1)  # Delay after each API call\n",
    "    else:\n",
    "        df.at[index, 'ai_description'] = \"\"  # Keep empty description if original was empty\n",
    "\n",
    "# Save the updated DataFrame back to the same file\n",
    "df.to_csv(input_file, index=False)\n",
    "print(\"Improved descriptions saved.\", input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File_Name renamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file has been saved to ../../data/product_data/shopify_urls_copy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '../../data/product_data/shopify_urls_copy.csv'  # Replace with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Function to standardize file_name\n",
    "def standardize_file_name(file_name):\n",
    "    # Regular expression to capture CODE, COLOR, and NUMBER\n",
    "    match = re.match(r\"^([A-Z0-9]+)_([A-Za-z-_]+)_(\\d+)\", file_name)\n",
    "    if match:\n",
    "        code = match.group(1)\n",
    "        color = match.group(2).replace('_', '-')  # Replace underscores with hyphens\n",
    "        number = match.group(3)\n",
    "\n",
    "        # Capitalize each word in color\n",
    "        color = '-'.join([word.upper() for word in color.split('-')])\n",
    "\n",
    "        # Reformat the file_name to CODE_COLOR_NUMBER\n",
    "        standardized_file_name = f\"{code}_{color}_{number} PNG\"\n",
    "        return standardized_file_name\n",
    "    else:\n",
    "        return file_name  # If it doesn't match, return the original\n",
    "\n",
    "# Apply the function to the 'file_name' column\n",
    "df['file_name'] = df['file_name'].apply(standardize_file_name)\n",
    "\n",
    "# Save the updated DataFrame back to the same file\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\"Updated file has been saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STYLE COLOR PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully in 'output.xlsx' with unique CODE and COLOR values.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from an Excel file\n",
    "input_file = '../data/product_data/main_data.csv'  # Replace with your actual file name\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Rename columns as specified\n",
    "df = df.rename(columns={\"STYLE NO.\": \"CODE\", \"COLOR\": \"COLOR\"})\n",
    "\n",
    "# Drop duplicate combinations of CODE and COLOR to get unique color entries for each CODE\n",
    "df_unique = df.drop_duplicates(subset=[\"CODE\", \"COLOR\"])\n",
    "\n",
    "# Select only the CODE and COLOR columns\n",
    "df_unique = df_unique[[\"CODE\", \"COLOR\"]]\n",
    "\n",
    "\n",
    "df_unique.to_csv(\"style_color_pictures.csv\", index=False)\n",
    "\n",
    "print(\"Data saved successfully in 'output.xlsx' with unique CODE and COLOR values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAPPING CODES,COLORS AND PICTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved as 'updated_codes_pictures.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file1 = pd.read_csv('./style_color_pictures.csv')  # Assuming the file with headers CODE, COLOR\n",
    "file2 = pd.read_csv('../data/product_data/shopify_urls.csv')  # Assuming the file with headers url, file_name, Code\n",
    "\n",
    "# Extract the color from file_name between the first and second underscores\n",
    "file2['color_in_file_name'] = file2['file_name'].str.extract(r'_(.*?)_')  # Extract color between underscores\n",
    "file2['Code_Color'] = file2['Code'].astype(str) + '_' + file2['color_in_file_name'].astype(str)\n",
    "\n",
    "# Add PHOTO and COUNT columns to file1 with default values\n",
    "file1['PHOTO'] = 'NO'\n",
    "file1['COUNT'] = 0\n",
    "file1['NOT IN INVENTORY'] = ''  # Initialize with empty strings\n",
    "\n",
    "# Process each row in file1 to check matches in file2\n",
    "for i, row in file1.iterrows():\n",
    "    code_color = f\"{row['CODE']}_{row['COLOR']}\"\n",
    "\n",
    "    # Check if there are any matches for this code_color in file2\n",
    "    matching_files = file2[file2['Code_Color'] == code_color]\n",
    "\n",
    "    if not matching_files.empty:\n",
    "        file1.at[i, 'PHOTO'] = 'YES'\n",
    "        file1.at[i, 'COUNT'] = len(matching_files)\n",
    "\n",
    "    # Check if there are any matches by CODE but with a different color in file2\n",
    "    unmatched_files = file2[(file2['Code'] == row['CODE']) & (file2['color_in_file_name'] != row['COLOR'])]\n",
    "\n",
    "    # If unmatched color found for an existing CODE, add it as a new row in file1 with 'NOT IN INVENTORY' as 'YES'\n",
    "    for _, unmatched_row in unmatched_files.iterrows():\n",
    "        new_color = unmatched_row['color_in_file_name']\n",
    "\n",
    "        # Check if this color does not already exist in file1 for the given code\n",
    "        if not ((file1['CODE'] == row['CODE']) & (file1['COLOR'] == new_color)).any():\n",
    "            new_row = {\n",
    "                'CODE': row['CODE'],\n",
    "                'COLOR': new_color,\n",
    "                'PHOTO': 'YES',\n",
    "                'COUNT': 1,\n",
    "                'NOT IN INVENTORY': 'YES'\n",
    "            }\n",
    "            file1 = pd.concat([file1, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Save the updated file\n",
    "file1.to_csv('updated_codes_pictures.csv', index=False)\n",
    "print(\"Updated file saved as 'updated_codes_pictures.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique codes in file1: 494\n",
      "Total unique codes in file2: 342\n",
      "Total codes in file1 but missing in file2: 152\n",
      "Missing codes: ['B8661', 'E1918', 'E5105', 'E2196', 'BD2007/10', 'E2111', 'B8667', 'E1449', 'E1877', 'AA9307', 'E2578', 'E2496', 'E1326', 'E1465', 'E1385', 'E1127', 'E1831', 'B8898', 'E2775', 'E1892', 'B8654', 'E1250', 'B8921', 'E1089', 'E1942', 'E1303', 'E1294', 'E2082', 'E1849', 'E1875', 'E1470', 'E2771', 'E2495', 'E1762', 'B8320', 'E1950', 'E2557', 'E1968', 'E2000', 'E2520', 'E1540', 'E2494', 'BD2006/344', 'E2599', 'B8468', 'E1064', 'E1745', 'BD2007/09', 'E2078', 'BD2006/66', 'E1605', 'B8486', 'E1334', 'E1333', 'E1442', 'B8506', 'E1068', 'B9000', 'E1552', 'E1636', 'BD2006/412', 'E1537', 'E2755', 'B8660', 'E2501', 'E2128', 'B8749', 'E1761', 'E2497', 'E2106', 'E2180', 'E1930', 'B8401', 'E1904', 'E1885', nan, 'E2301', 'B8751', 'E1909', 'BD2007/417', 'E2610', 'E2524', 'E1879', 'B8716', 'E2503', 'E1538', 'E1024', 'E2778', 'E1278', 'B8896', 'BD2007/182', 'E1593', 'E1946', 'BD2007/183', 'E1939', 'B8894', 'B8691', 'E1908', 'BD2006/413', 'E2201', 'E1954', 'E2781', 'B8621', 'BD2006/792', 'E1497', 'E1893', 'E2605', 'E2717', 'BD2007/101', 'E2126', 'AA9271', 'E1387', 'E1948', 'BD2006/502', 'E2011', 'E2057', 'E1809', 'E1947', '42113', 'E1597', 'BD2006/741', 'E1507', 'BD2006/24', 'B8940', 'B1194', 'E2018', 'B8967', 'BD2006/183', 'E1503', 'E1018', 'E2542', 'BD2006/200', 'E1044', 'E2073', 'B8472', 'E1119', 'E2400', 'E1676', 'E1194', 'E1895', 'E1980', 'E1683', 'E1065', 'E2787', 'E1716', 'E1412', 'E24', 'B8622', '4386', 'E2779', 'E1025', 'E1861']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = pd.read_csv('./style_color_pictures.csv')  # Assuming the file with headers CODE, COLOR\n",
    "file2 = pd.read_csv('../data/product_data/shopify_urls.csv')  # Assuming the file with headers url, file_name, Code\n",
    "\n",
    "# Get unique codes from each file\n",
    "codes_file1 = set(file1['CODE'])\n",
    "codes_file2 = set(file2['Code'])\n",
    "\n",
    "# Calculate totals\n",
    "total_styles_file1 = len(codes_file1)\n",
    "total_styles_file2 = len(codes_file2)\n",
    "\n",
    "# Calculate codes missing in file2 but present in file1\n",
    "missing_in_file2 = codes_file1 - codes_file2\n",
    "missing_count = len(missing_in_file2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total unique codes in file1: {total_styles_file1}\")\n",
    "print(f\"Total unique codes in file2: {total_styles_file2}\")\n",
    "print(f\"Total codes in file1 but missing in file2: {missing_count}\")\n",
    "print(f\"Missing codes: {list(missing_in_file2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
