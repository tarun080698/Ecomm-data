{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1: Get the Style code, color, size, quantity, brand and category from the main sheet into a new sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Codes in the original DataFrame:  476\n",
      "Selected columns have been written to ../data/main/filtered_data.csv\n",
      "177\n",
      "Invalid rows have been written to ../data/main/invalid_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = '../data/product_data/main_data.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Rename specific columns\n",
    "df = df.rename(columns={\n",
    "    'STYLE NO.': 'Code',\n",
    "    'SIZE_US': 'Size',\n",
    "    'COLOR': 'Color',\n",
    "    'QUANTITY': 'Quantity',\n",
    "    'BRAND NAME': 'Brand',\n",
    "    # 'CATEGORIES': 'Category',\n",
    "})\n",
    "\n",
    "# Define validation criteria for each column\n",
    "\n",
    "\n",
    "def validate_row(row):\n",
    "    # Check for missing or blank values\n",
    "    if pd.isnull(row['Brand']) or pd.isnull(row['Code']) or pd.isnull(row['Color']) or pd.isnull(row['Quantity']) or pd.isnull(row['Size']):\n",
    "        return False\n",
    "    if row['Brand'].strip() == '' or row['Code'].strip() == '' or row['Color'].strip() == '':\n",
    "        return False\n",
    "\n",
    "    # Check for correct data types\n",
    "    if not isinstance(row['Brand'], str) or not isinstance(row['Code'], str) or not isinstance(row['Color'], str):\n",
    "        return False\n",
    "    try:\n",
    "        float(row['Quantity'])\n",
    "        float(row['Size'])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Apply validation to each row\n",
    "valid_rows = df.apply(validate_row, axis=1)\n",
    "\n",
    "# Filter the DataFrame to retain only valid rows\n",
    "filtered_df = df[valid_rows]\n",
    "\n",
    "# Select the specified columns based on the new names\n",
    "# selected_columns = ['Code', 'Brand', 'Color', 'Quantity', 'Size', 'Category']\n",
    "selected_columns = ['Code', 'Brand', 'Color', 'Quantity', 'Size']\n",
    "new_df = filtered_df[selected_columns]\n",
    "\n",
    "# Write the filtered data to a new CSV file\n",
    "# output_file = '../../scripts/catalog/filtered_data.csv'\n",
    "output_file = '../data/main/filtered_data.csv'\n",
    "new_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Print number of unique Codes\n",
    "print(\"Number of unique Codes in the original DataFrame: \",new_df['Code'].nunique())\n",
    "print(f\"Selected columns have been written to {output_file}\")\n",
    "\n",
    "\n",
    "# Identify rows that were dropped\n",
    "invalid_rows = df[~valid_rows]\n",
    "\n",
    "output_file_invalid = '../data/main/invalid_data.csv'\n",
    "invalid_rows.to_csv(output_file_invalid, index=False)\n",
    "\n",
    "# Print number of unique Codes\n",
    "print(invalid_rows['Code'].nunique())\n",
    "print(f\"Invalid rows have been written to {output_file_invalid}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 2: Group this sheet by style code, color, and size and sort all these by asc order, sum of quantity by group to get the total quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Codes in the original DataFrame: 476\n",
      "Number of unique Codes after aggregation: 476\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(\"../data/main/filtered_data.csv\")\n",
    "\n",
    "# Ensure Quantity is numeric, coercing errors to NaN\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
    "\n",
    "# Fill NaN values in Quantity with 0 (or handle as needed)\n",
    "df['Quantity'] = df['Quantity'].fillna(0)\n",
    "\n",
    "# First aggregation: Sum quantities based on Code, Color, and Size\n",
    "aggregated_df = df.groupby(['Code', 'Color', 'Size'], as_index=False).agg({\n",
    "    'Brand': 'first',  # Assuming you want to keep the first Brand name found\n",
    "    'Quantity': 'sum',\n",
    "    # 'Category': 'first'\n",
    "})\n",
    "\n",
    "# Sort the aggregated data by Code, Color, and Size in ascending order\n",
    "aggregated_df = aggregated_df.sort_values(by=['Code', 'Color', 'Size'], ascending=[True, True, True])\n",
    "\n",
    "\n",
    "# Print the number of unique codes after aggregation\n",
    "print(f\"Number of unique Codes in the original DataFrame: {df['Code'].nunique()}\")\n",
    "print(f\"Number of unique Codes after aggregation: {aggregated_df['Code'].nunique()}\")\n",
    "\n",
    "# Write the aggregated data to a new CSV file\n",
    "aggregated_df.to_csv('../data/main/step_2_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3: Get the prices and Description of available product codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476\n",
      "353\n",
      "The updated file with Price and Description added: ../data/main/step_3_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load file A and file B\n",
    "original_file = \"../data/main/step_2_output.csv\"\n",
    "data_file = \"../data/product_data/product_data.csv\"\n",
    "df_original_file = pd.read_csv(original_file)\n",
    "df_data_file = pd.read_csv(data_file)\n",
    "\n",
    "# Merge file A with file B on the \"Code\" column, keeping all rows from file A\n",
    "merged_df = pd.merge(df_original_file, df_data_file[['Code', 'Description', 'Price', 'ai_description','Category','Collection','Type']], on='Code', how='left')\n",
    "\n",
    "print(merged_df['Code'].nunique())\n",
    "# # Drop rows where Price is NaN or 0\n",
    "# merged_df = merged_df.dropna(subset=['Price'])\n",
    "# merged_df = merged_df[merged_df['Price'] != 0]\n",
    "\n",
    "# Drop rows where Price is NaN\n",
    "merged_df = merged_df.dropna(subset=['Price'])\n",
    "\n",
    "\n",
    "# print(merged_df['Code'].nunique())\n",
    "# merged_df = merged_df[merged_df['Price'] >= 150]\n",
    "\n",
    "\n",
    "print(merged_df['Code'].nunique())\n",
    "# Save the result to a new CSV file\n",
    "output_file = \"../data/main/step_3_output.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"The updated file with Price and Description added: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 5: Combine image URLs with the product code and add additional URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV file with sorted URLs has been saved to ../data/main/step_5_output.csv\n",
      "Number of unique handles: 307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarun\\AppData\\Local\\Temp\\ipykernel_28788\\3670805004.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: sort_file_names(x['file_name'].tolist(), x['url'].tolist()))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV files\n",
    "file1_path = '../data/main/step_3_output.csv'\n",
    "file2_path = '../data/product_data/shopify_urls.csv'  # Replace with the actual file path\n",
    "\n",
    "file1 = pd.read_csv(file1_path)\n",
    "file2 = pd.read_csv(file2_path)\n",
    "\n",
    "# Group URLs by 'Code' and create a sorted list of URLs for each 'Code' based on 'file_name'\n",
    "def sort_file_names(file_name_list, url_list):\n",
    "    # Extract color and number suffix for sorting\n",
    "    def get_sort_key(file_name):\n",
    "        match = re.search(r'_([A-Za-z]+)_(\\d+)', file_name)\n",
    "        if match:\n",
    "            color = match.group(1)\n",
    "            number = int(match.group(2))\n",
    "            return color, number\n",
    "        return \"\", float('inf')  # Default sort key if no match is found\n",
    "\n",
    "    # Sort both file names and URLs based on the file_name sort key\n",
    "    sorted_files_with_urls = sorted(zip(file_name_list, url_list), key=lambda x: get_sort_key(x[0]))\n",
    "    sorted_urls = [url for _, url in sorted_files_with_urls]\n",
    "    return ','.join(sorted_urls)\n",
    "\n",
    "# Group and sort URLs for each Code based on 'file_name'\n",
    "url_grouped = (\n",
    "    file2.groupby('Code')\n",
    "    .apply(lambda x: sort_file_names(x['file_name'].tolist(), x['url'].tolist()))\n",
    "    .reset_index(name='Urls')\n",
    ")\n",
    "\n",
    "# Merge the grouped and sorted URLs back to the original DataFrame\n",
    "final_df = pd.merge(file1, url_grouped, on='Code', how='left')\n",
    "\n",
    "# Discard rows where 'Urls' is NaN (no URL available)\n",
    "final_df = final_df.dropna(subset=['Urls'])\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "output_file_path = '../data/main/step_5_output.csv'\n",
    "final_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Final CSV file with sorted URLs has been saved to {output_file_path}\")\n",
    "print(f\"Number of unique handles: {final_df['Code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 6: Creating shopify csv file to upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307\n",
      "['IVORY' nan 'WHITE' 'WHITE-IVORY' 'APPLE-RED' 'BURGUNDY' 'LAVENDER'\n",
      " 'WHITE-BURGUNDY' 'WHITE-NAVY' 'WHITE-RED' 'LILAC' 'PINK' 'SKY-BLUE'\n",
      " 'BLACK' 'IVORY-BLACK' 'IVORY-NAVY' 'NAVY-IVORY' 'TRUFFLE' 'BROWN'\n",
      " 'CLARET' 'POOL' 'PURPLE' 'ROYAL' 'SAGE' 'SANGRIA' 'SILVER' 'CHAMPAGNE'\n",
      " 'CORAL' 'LIGHT-GREEN' 'RED' 'WINE' 'FUCHSIA' 'SUNSHINE' 'SUNRISE'\n",
      " 'PISTACHIO' 'PEACH-FIZZ' 'NAVY' 'TEAL' 'VICTORIA-LILAC' 'GOLD' 'JADE'\n",
      " 'ORANGE' 'BLUE' 'COFFEE' 'GREEN' 'PEACH' 'DARK-LILAC' 'CAFE' 'WATERMELON'\n",
      " 'WHITE-BLACK' 'YELLOW' 'PLATINUM' 'APPLERED-WHITE' 'TRUFFLE-WHITE'\n",
      " 'PEACOCK' 'PLUM' 'SEA-BLACK' 'GREY' 'VICTORIA' 'EMERALD' 'PURPLE-BLACK'\n",
      " 'NUDE' 'WHITE-BLUE' 'BURNT-ORANGE' 'BLACK-NUDE' 'BLUE-PINK' 'BANANA'\n",
      " 'MINT' 'PRINT' 'CHARCOAL' 'BLACK-SILVER' 'POWDER-BLUE' 'BISCOTTI'\n",
      " 'PURPLE-GOLD' 'GUNMETAL' 'MIDNIGHT' 'POOL-WHITE' 'ROYAL-BLUE' 'TIFFANY'\n",
      " 'TURQUOISE' 'JET-COBALT' 'BERRY' 'BLACK-GUNMETAL' 'JET' 'ROSE-QUARTZ'\n",
      " 'ROSE' 'BLUE-BLACK' 'BLACK-BURGUNDY' 'FLORAL-PRINT' 'APRICOT' 'COBALT'\n",
      " 'DIRTY-ROSE' 'DUSTY-ROSE' 'WHITE-PRINT' 'RAINBOW' 'LIGHT-BLUE'\n",
      " 'LIGHTBLUE' 'BLACK-GREEN' 'BLACK-RED' 'RED-BLACK' 'RED-' 'BLACK-GOLD'\n",
      " 'GREEN-PURPLE' 'MULTI' 'BLUSH' 'PEAL' 'BLACK-PRINT' 'PURPLE-PRINT'\n",
      " 'IVORY-PRINT' 'MUD' 'NUDE-IVORY' 'NAVY-PRINT' 'AQUA' 'MAUVE' 'LIGHT-GOLD'\n",
      " 'BANANA-FLOWER' 'BANANA-YELLOW' 'DARK-GREEN' 'PERIWINKLE' 'PERRI']\n",
      "Shopify CSV file has been saved to ../data/shopify_sheets/shopify_final_sheet_12-22-2024_21-40.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "file1_path = '../data/main/step_5_output.csv'\n",
    "file1 = pd.read_csv(file1_path)\n",
    "\n",
    "# Shopify CSV structure\n",
    "shopify_columns = [\n",
    "    'Handle', 'Title', 'Body (HTML)', 'Vendor', 'Type', 'Tags', 'Published',\n",
    "    'Option1 Name', 'Option1 Value', 'Option2 Name', 'Option2 Value',\n",
    "    'Variant SKU', 'Variant Grams', 'Variant Inventory Tracker',\n",
    "    'Variant Inventory Qty', 'Variant Inventory Policy',\n",
    "    'Variant Fulfillment Service', 'Variant Price',\n",
    "    'Variant Compare at Price', 'Variant Requires Shipping', 'Variant Taxable',\n",
    "    'Variant Barcode', 'Image Src', 'Image Position', 'Image Alt Text',\n",
    "    'Variant Image'  # New column for variant-specific image\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "shopify_df = pd.DataFrame(columns=shopify_columns)\n",
    "all_entries = []\n",
    "current_code = None\n",
    "other_images = []\n",
    "count = 0\n",
    "\n",
    "# Iterate over rows in the CSV\n",
    "for index, row in file1.iterrows():\n",
    "\n",
    "    # Function to handle NaN values safely\n",
    "    def safe_str(value):\n",
    "        return str(value) if pd.notna(value) else \"\"\n",
    "\n",
    "    def replace_commas(*args):\n",
    "        if all(not arg for arg in args):\n",
    "            return \"\"\n",
    "        return '; '.join(f'\"{arg.replace(\",\", \";\")}\"' for arg in args)\n",
    "\n",
    "    def combine_tags(brand=\"\", collection=\"\", category=\"\", dress_length=\"\"):\n",
    "        def safe_str(value):\n",
    "            # Convert to string and strip whitespace; return empty string if None or empty\n",
    "            return str(value).strip() if value and str(value).strip() else \"\"\n",
    "\n",
    "        # Create a list of values, conditionally adding \"Wedding\" based on brand\n",
    "        values = [\n",
    "            \"Wedding\" if safe_str(brand) == \"ANDY ANAND COUTURE\" else \"\",\n",
    "            safe_str(collection),\n",
    "            safe_str(category),\n",
    "            safe_str(dress_length)\n",
    "        ]\n",
    "\n",
    "        # Filter out empty values and remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_values = [v for v in values if v and v not in seen and not seen.add(v)]\n",
    "\n",
    "        # Join unique values with commas\n",
    "        result = \", \".join(unique_values)\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Check if we have encountered a new code\n",
    "    if current_code is not None and current_code != row['Code']:\n",
    "        # Append additional images for the previous code\n",
    "        for idx, url in enumerate(other_images):\n",
    "            image_entry = {\n",
    "                'Handle': current_code.lower(),\n",
    "                'Image Src': url,\n",
    "                'Image Position': idx + 2,\n",
    "            }\n",
    "            all_entries.append(pd.DataFrame([image_entry]))\n",
    "        other_images = []  # Reset for new code\n",
    "\n",
    "    current_code = safe_str(row['Code'])  # Set the current code safely\n",
    "    # if current_code.lower() != 'e1264':\n",
    "    #     continue\n",
    "\n",
    "    # Prepare product description\n",
    "\n",
    "    def generate_body_html(ai_description, description):\n",
    "        def safe_str(value):\n",
    "            return str(value).strip() if value and str(value).strip() else \"\"\n",
    "\n",
    "        # Check conditions for generating body_html\n",
    "        if safe_str(ai_description):  # Use ai_description if it is not empty\n",
    "            body_html = f\"<p>{safe_str(ai_description)}</p>\"\n",
    "        elif safe_str(description):  # Use Description if ai_description is empty\n",
    "            body_html = safe_str(description)\n",
    "        else:  # Return an empty string if both are empty\n",
    "            body_html = \"\"\n",
    "\n",
    "        return body_html\n",
    "\n",
    "    body_html = generate_body_html(safe_str(row['ai_description']), safe_str(row['Description']))\n",
    "\n",
    "    def limit_seo_description(text, limit=300):\n",
    "        return text[:limit] if len(text) > limit else text\n",
    "\n",
    "    # Extract and filter image URLs based on the color\n",
    "    if isinstance(row['Urls'], str):\n",
    "        all_images = row['Urls'].split(\",\")\n",
    "        # Filter images for the variant's color\n",
    "        color = safe_str(row['Color']).upper()\n",
    "        color_images = [\n",
    "            img for img in all_images if color in img.upper()\n",
    "        ]\n",
    "\n",
    "        # Sort images by the number suffix (front, back, additional)\n",
    "        color_images.sort(key=lambda x: int(re.search(r'_(\\d+)', x).group(1)) if re.search(r'_(\\d+)', x) else 99)\n",
    "\n",
    "        # Assign images based on the suffix meaning\n",
    "        first_image = color_images[0] if color_images else all_images[0]  # Fallback to the first URL in all_images if color_images is empty\n",
    "        other_images = color_images[1:] if len(color_images) > 1 else []\n",
    "\n",
    "        # Set the 'Variant Image' as the primary color image (suffix `_1`)\n",
    "        variant_image = next((img for img in color_images if \"_1\" in img), first_image)\n",
    "    else:\n",
    "        first_image = \"\"\n",
    "        variant_image = \"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Create a new product entry\n",
    "    product_entry = {\n",
    "        'Handle': current_code.lower(),\n",
    "        'Title': f\"{safe_str(row['Brand'])} {current_code}\",\n",
    "        'Body (HTML)': body_html,\n",
    "        'Vendor': \"Nova Vogue\",\n",
    "        'Type': safe_str(row['Type']) + \" Dress\",\n",
    "        'Tags': combine_tags(row['Brand'],row['Collection'],row['Category'], row['Type']),\n",
    "        'Published': True,\n",
    "        'Product Category': \"Apparel & Accessories > Clothing > Dresses\",\n",
    "        'Option1 Name': 'Color',\n",
    "        'Option1 Value': safe_str(row['Color']),\n",
    "        'Option2 Name': 'Size',\n",
    "        'Option2 Value': safe_str(row['Size']),\n",
    "        'Variant SKU': f\"{current_code}_{safe_str(row['Color']).upper()}_{safe_str(row['Size'])}\",\n",
    "        'Variant Grams': '',\n",
    "        'Variant Inventory Tracker': 'shopify',\n",
    "        'Variant Inventory Qty': safe_str(row['Quantity']),\n",
    "        'Variant Inventory Policy': 'deny',\n",
    "        'Variant Fulfillment Service': 'manual',\n",
    "        'Variant Price': safe_str(row['Price']),\n",
    "        'Variant Compare at Price': '',\n",
    "        'Variant Requires Shipping': 'TRUE',\n",
    "        'Variant Taxable': 'TRUE',\n",
    "        'Variant Barcode': '',\n",
    "        'Image Src': first_image,\n",
    "        'Image Position': 1,\n",
    "        'Image Alt Text': f\"Nova Vogue CODE: {current_code}_{safe_str(row['Color']).upper()}_{safe_str(row['Size'])}\",\n",
    "        'Variant Image': variant_image,  # Add Variant Image field\n",
    "        # SEO\n",
    "        'SEO Title': f\"{safe_str(row['Brand'])} {current_code}\",\n",
    "        'SEO Description': limit_seo_description(body_html),\n",
    "        'Google Shopping / Google Product Category': \"Apparel & Accessories > Clothing > Dresses\",\n",
    "        'Google Shopping / Gender': 'Female',\n",
    "        'Google Shopping / Age Group': 'Adult',\n",
    "        'Google Shopping / AdWords Grouping': 'Women Dresses',\n",
    "        'Google Shopping / AdWords Labels': combine_tags(row['Brand'],row['Collection'],row['Category'], row['Type']),\n",
    "        'Google Shopping / Condition': 'new',\n",
    "        'Google Shopping / Custom Product': False,\n",
    "        'Google Shopping / Custom Label 0': safe_str(row['Brand']),\n",
    "        'Google Shopping / Custom Label 1': safe_str(row['Collection']),\n",
    "        'Google Shopping / Custom Label 2': safe_str(row['Category']),\n",
    "        'Google Shopping / Custom Label 3': safe_str(row['Type']),\n",
    "        'Google Shopping / Custom Label 4': safe_str(row['Color']).upper() + ' ' +safe_str(row['Size']),\n",
    "        # metafields\n",
    "        'product.metafields.custom.color': safe_str(row['Color']),\n",
    "        'product.metafields.custom.size': safe_str(row['Size']),\n",
    "        'product.metafields.custom.dress_occasion': replace_commas(safe_str(row['Category']), safe_str(row['Collection']), safe_str(row['Type'])),\n",
    "        'product.metafields.custom.dress_style': replace_commas(safe_str(row['Category']), safe_str(row['Collection']), safe_str(row['Type'])),\n",
    "        'product.metafields.custom.skirt_length_type': safe_str(row['Type']),\n",
    "        'product.metafields.custom.sleeve_length_type': safe_str(row['Category']),\n",
    "    }\n",
    "\n",
    "    # Add the main product entry to the list\n",
    "    all_entries.append(pd.DataFrame([product_entry]))\n",
    "\n",
    "    # Append additional images for the last code\n",
    "    if other_images:\n",
    "        for idx, url in enumerate(other_images):\n",
    "            image_entry = {\n",
    "                'Handle': current_code.lower(),\n",
    "                'Image Src': url,\n",
    "                'Image Position': idx + 2,\n",
    "            }\n",
    "            all_entries.append(pd.DataFrame([image_entry]))\n",
    "\n",
    "# Concatenate all entries into a single DataFrame\n",
    "shopify_df = pd.concat(all_entries, ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "output_file_path = f'../data/shopify_sheets/shopify_final_sheet_{datetime.now().strftime(\"%m-%d-%Y_%H-%M\")}.csv'\n",
    "\n",
    "shopify_df.to_csv(output_file_path, index=False, sep=\",\")\n",
    "\n",
    "# Print the number of unique handles\n",
    "print(shopify_df['Handle'].nunique())\n",
    "print(shopify_df['Option1 Value'].unique())\n",
    "\n",
    "print(f\"Shopify CSV file has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fixing 570 for top 100 variants\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '../data/shopify_sheets/shopify_final_sheet_12-10-2024_23-42.csv'  # Replace with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to filter and prioritize the variants\n",
    "def get_100_variants(df):\n",
    "    # Sort by inventory quantity (descending) and size (ascending)\n",
    "    df_sorted = df.sort_values(\n",
    "        by=['Variant Inventory Qty', 'Option2 Value'],  # Sort by quantity first, then size\n",
    "        ascending=[False, True]\n",
    "    )\n",
    "\n",
    "    # Filter only the unique handle (F570)\n",
    "    df_filtered = df_sorted[df_sorted['Handle'] == 'F570']\n",
    "\n",
    "    # Initialize result list\n",
    "    result = []\n",
    "\n",
    "    # Get all unique colors\n",
    "    unique_colors = df_filtered['Option1 Value'].unique()\n",
    "\n",
    "    # Add at least one size for each color\n",
    "    for color in unique_colors:\n",
    "        color_group = df_filtered[df_filtered['Option1 Value'] == color]\n",
    "\n",
    "        # Add the size with the highest quantity for this color\n",
    "        top_size = color_group.iloc[0]\n",
    "        result.append(top_size.to_dict())\n",
    "\n",
    "        # Add additional sizes for the color if space allows\n",
    "        additional_sizes = color_group.iloc[1:]  # Exclude the first (already added)\n",
    "        for _, row in additional_sizes.iterrows():\n",
    "            result.append(row.to_dict())\n",
    "\n",
    "    # If we still have less than 100 rows, keep adding rows to reach 100\n",
    "    result_df = pd.DataFrame(result)\n",
    "    while len(result_df) < 100:\n",
    "        for color in unique_colors:\n",
    "            color_group = df_filtered[df_filtered['Option1 Value'] == color]\n",
    "            for _, row in color_group.iterrows():\n",
    "                if len(result_df) < 100:\n",
    "                    result_df = result_df.append(row, ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    # Ensure only the top 100 rows are returned\n",
    "    return result_df.head(100)\n",
    "\n",
    "# Apply the function\n",
    "filtered_products = get_100_variants(df)\n",
    "\n",
    "# Save or display the results\n",
    "output_file = 'exactly_100_variants.csv'\n",
    "filtered_products.to_csv(output_file, index=False)\n",
    "print(f\"Filtered products saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 100 rows saved to grouped_dresses.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = \"../data/shopify_sheets/570.csv\"  # Update this with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check if the required columns exist\n",
    "if \"Option1 Value\" not in df.columns or \"Option2 Value\" not in df.columns:\n",
    "    raise ValueError(\"CSV must contain 'color' and 'size' columns\")\n",
    "\n",
    "# Group by color\n",
    "groups = df.groupby(\"Option1 Value\")\n",
    "\n",
    "# Initialize variables\n",
    "result = []\n",
    "colors = list(groups.groups.keys())\n",
    "color_indices = {color: 0 for color in colors}  # Track index for each color\n",
    "\n",
    "# Loop until we have 100 rows\n",
    "while len(result) < 100:\n",
    "    for color in colors:\n",
    "        group = groups.get_group(color)  # Get the group for the color\n",
    "\n",
    "        # Check if there are remaining rows for this color\n",
    "        if color_indices[color] < len(group):\n",
    "            row = group.iloc[color_indices[color]]  # Get the next row for the color\n",
    "            result.append(row)\n",
    "            color_indices[color] += 1  # Move to the next row for this color\n",
    "\n",
    "            if len(result) >= 100:\n",
    "                break\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "final_df = pd.DataFrame(result)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "output_file = \"grouped_dresses.csv\"\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\"Final 100 rows saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped data saved to F570_color_size_Uploads.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "# file_path = '../data/shopify_sheets/570.csv'  # Replace with the actual file path\n",
    "# df = pd.read_csv(file_path)\n",
    "file_path_1 = '570-uploads.csv'  # Replace with the actual file path\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "\n",
    "# Check if the required columns exist\n",
    "if \"Color\" not in df.columns or \"Size\" not in df.columns or \"Code\" not in df.columns or \"Quantity\" not in df.columns:\n",
    "    raise ValueError(\"CSV must contain 'Code', 'Color', 'Size', and 'Quantity' columns\")\n",
    "\n",
    "# Group by Color, Size, and Code\n",
    "grouped = df.groupby([\"Color\", \"Size\", \"Code\"])\n",
    "\n",
    "# Convert the grouped data into a new DataFrame\n",
    "result = grouped.size().reset_index(name='Count')\n",
    "\n",
    "# Save the grouped data to a new CSV file\n",
    "output_file = \"F570_color_size_Uploads.csv\"\n",
    "result.to_csv(output_file, index=False)\n",
    "print(f\"Grouped data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
